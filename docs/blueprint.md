**Zero-Code User Blueprint for SPARC Program Generation**

**Project Title:**
*Your Answer (following iterative refinement):* **NexusInsight: Your Superior Wikidata Navigator**
*(This title suggests connection (Nexus), deep understanding (Insight), and ease of use (Navigator), reflecting the client's goal to be more than just an explorer, powered by the intelligent capabilities of the MCP server.)*

**Prepared By:**
*Nate Aune:* AI Product Architect (LLM-Assisted)

**Date:**
May 11, 2025

**Instructions for You (The Visionary!):**
• No Tech Jargon Needed! Just describe your idea in plain English. Think about what you want the program to do and why, not how it does it technically.
• Be Detailed: The more information and specific examples you give, the better the AI (our team of virtual coding assistants, called SPARC) can understand and build exactly what you want. Imagine you're describing it to someone who needs to build it perfectly without asking you follow-up questions.
• Focus on the Goal: What problem does this solve? What process does it make easier?
• Don't Worry About Code: SPARC will figure out the best programming languages, databases, and technical stuff based on your description and its own research.

**Section 1: The Big Picture - What is this program all about?**

1.  **Elevator Pitch:** If you had 30 seconds to describe your program to a friend, what would you say? What's the main goal?
    *   *Your Answer (following iterative refinement):* Imagine effortlessly unlocking the rich stories and hidden connections within the world's largest open knowledge base, Wikidata. **NexusInsight** makes exploring this vast universe of information intuitive, insightful, and even fun for everyone, from students needing quick facts to researchers uncovering deep patterns. It achieves this by intelligently organizing, simplifying, and beautifully presenting complex data, all thanks to its connection with a powerful `wikidata-mcp` backend server that does the heavy lifting. The main goal is to transform how people interact with and benefit from Wikidata.

2.  **Problem Solver:** What specific problem does this program solve, or what task does it make much easier or better?
    *   *Your Answer (following iterative refinement):* This program solves several key problems:
        *   **Complexity Barrier:** Wikidata is incredibly powerful but notoriously difficult for non-technical users to query directly (e.g., using SPARQL). NexusInsight removes this barrier.
        *   **Information Overload:** Finding specific information or understanding the "big picture" in Wikidata can be like searching for a needle in a haystack. NexusInsight curates and presents information clearly.
        *   **Discovering Connections:** It's hard to see how different pieces of information relate to each other. NexusInsight excels at visualizing these relationships.
        *   **Slow or Unintuitive Interfaces:** Existing tools can be slow, clunky, or overwhelming. NexusInsight offers a fast, modern, and user-friendly experience.
        The `wikidata-mcp` server is crucial here, as it pre-processes data, simplifies query mechanisms, and optimizes delivery, allowing NexusInsight to focus on superior user experience and insight generation.

3.  **Why Does This Need to Exist?** What's the key benefit it offers? (e.g., saves time, saves money, organizes information, connects people, provides entertainment, etc.)
    *   *Your Answer (following iterative refinement):* NexusInsight needs to exist to **democratize access to structured global knowledge and empower users to derive meaningful insights, not just retrieve isolated facts.** Key benefits include:
        *   **Saves Significant Time:** Radically reduces time spent finding and understanding Wikidata information.
        *   **Enhances Understanding:** Helps users grasp complex topics and relationships they might otherwise miss. The `wikidata-mcp` server can provide curated views and summaries to aid this.
        *   **Unlocks New Discoveries:** Enables users to explore data in novel ways, potentially leading to new insights or research directions.
        *   **Broadens Usability:** Makes Wikidata valuable to a much wider audience beyond technical specialists.
        *   **Improves Data-Driven Decision Making:** For professionals, it provides easier access to a rich dataset for analysis and context.

**Section 2: The Users - Who is this program for?**

1.  **Primary Users:** Describe the main type of person (or people) who will use this program. (e.g., small business owners, students, hobbyists, families, everyone, etc.)
    *   *Your Answer (following iterative refinement):* NexusInsight is designed for a diverse range of users, including:
        *   **Students (High School, College, University):** For research, assignments, and general learning.
        *   **Researchers & Academics:** For literature reviews, data gathering, and exploring connections in their fields.
        *   **Journalists & Content Creators:** For fact-checking, background research, and finding story ideas.
        *   **Educators:** To help teach concepts using real-world connected data.
        *   **Librarians & Archivists:** For exploring and contextualizing collections.
        *   **Curious Individuals & Lifelong Learners:** Anyone interested in exploring knowledge in an engaging way.
        *   **Data Analysts (non-Wikidata experts):** Who need easy access to structured data from Wikidata without learning SPARQL.
        The `wikidata-mcp` server's ability to simplify data access will be key to serving both novice and more advanced users within these groups effectively.

2.  **User Goals:** When someone uses your program, what are the top 1-3 things they want to accomplish with it?
    *   *Your Answer (following iterative refinement):*
        ▪ 1. **Effortlessly Find Specific Information:** Quickly get accurate, human-readable answers to their questions about entities, facts, and concepts (e.g., "When was Marie Curie born?", "What are the major works of Shakespeare?"). The `wikidata-mcp` server will ensure these queries are fast and results are well-structured.
        ▪ 2. **Explore and Understand Connections:** Intuitively navigate and visualize the relationships between different topics, people, places, events, and concepts to gain a deeper understanding of context and influence. The `wikidata-mcp`'s pre-processed graph data will power these explorations.
        ▪ 3. **Discover and Learn Something New:** Go beyond targeted searching to serendipitously discover interesting facts, patterns, or new areas of knowledge they weren't initially looking for, guided by intelligent suggestions and visualizations.
        ▪ 4. **Gather and Organize Information for a Purpose:** Easily collect relevant data points, lists, or even simple visualizations for use in their own projects, reports, or presentations, with easy export options facilitated by the `wikidata-mcp` server's structured outputs.
        ▪ 5. **Verify or Triangulate Information:** Quickly check facts or find supporting data from a large, interconnected knowledge base. The `wikidata-mcp` server might offer features like access to data versioning or source prominence which could aid this.

**Section 3: The Features - What can the program do?**

1.  **Core Actions:** List the essential actions or tasks users can perform within the program. Be specific. Use action words.
    *   *Your Answer (List as many as needed, following iterative refinement):*
        ▪ **Search Intelligently:** Find entities (people, places, concepts, etc.) using natural language, keywords, or even by describing characteristics. The `wikidata-mcp` server will power an advanced, multilingual semantic search.
        ▪ **View Comprehensive Item Pages:** See a clearly organized, human-readable page for any Wikidata item, displaying key facts, descriptions, images, and important relationships, all pulled from structured data provided by the `wikidata-mcp` server.
        ▪ **Explore Connections Visually:** Navigate an interactive graph or network diagram showing how items are related, with options to expand nodes and filter by relationship types. The `wikidata-mcp` server will provide the optimized graph data for this.
        ▪ **Build Queries Visually:** Construct complex questions without writing code, using dropdowns, selectors, and a guided interface that translates user intent into efficient queries for the `wikidata-mcp` server.
        ▪ **Generate Dynamic Visualizations:** Create charts, timelines, maps, and other visual representations of query results or item comparisons, using data processed and served by `wikidata-mcp`.
        ▪ **Compare Items Side-by-Side:** Easily view detailed comparisons of two or more Wikidata items, highlighting similarities and differences.
        ▪ **Filter and Sort Results:** Refine search results or lists of items based on various criteria (e.g., item type, date ranges, specific properties) handled efficiently by `wikidata-mcp` queries.
        ▪ **Trace Data Lineage (Basic):** View links back to original Wikidata items to understand data sources (facilitated by `wikidata-mcp` passing through necessary metadata).
        ▪ **Export Data Simply:** Download information (search results, item details, visualization data) in common, usable formats like CSV, JSON, or image files for charts, prepared by the `wikidata-mcp` server.
        ▪ **Save & Share Discoveries:** Bookmark interesting items, queries, or visualizations for later, and share links to specific views with others.
        ▪ **Personalize Experience:** Set preferred languages for labels and descriptions, customize dashboard views.
        ▪ **Receive Contextual Suggestions:** Get recommendations for related items or interesting exploration paths based on current activity, potentially powered by `wikidata-mcp` analytics or pre-computed associations.

2.  **Key Feature Deep Dive:** Pick the MOST important feature from your list above. Describe step-by-step how you imagine someone using that specific feature from start to finish. What do they see? What do they click? What happens next?
    *   *Your Answer (following iterative refinement, ensuring this highlights superiority and MCP leverage):*
        **Feature: Interactive Visual Exploration of Connections**

        This feature allows users to intuitively understand the web of relationships around any Wikidata item, making it vastly superior to just reading lists of facts. It heavily relies on the `wikidata-mcp` server for fast, structured graph data.

        **Step-by-Step Usage:**
        1.  **Initiation:** The user types "Leonardo da Vinci" into a prominent, smart search bar at the top of the screen. As they type, suggestions from the `wikidata-mcp` server's enhanced search appear. They select "Leonardo da Vinci (Q762)" from the suggestions.
        2.  **Initial Display - The Hub:** The main area of the screen transforms into a visual graph. A central, visually distinct "node" representing Leonardo da Vinci appears. Around it, several clearly labeled clusters of connected nodes automatically appear, perhaps organized by common relationship types (e.g., "His Artworks," "People He Knew," "Places He Lived," "His Ideas/Fields of Study"). These initial connections are fetched via a highly optimized `wikidata-mcp` API call (e.g., `/entity/Q762/connections?depth=1&common_groups=true`). Each node might have a small representative icon or image if available from the `wikidata-mcp`.
        3.  **Hover for Quick Info:** The user hovers their mouse over the "Mona Lisa" node within the "His Artworks" cluster. A small, non-intrusive pop-up appears instantly (data fetched quickly from `wikidata-mcp` cache or a lightweight endpoint), showing a thumbnail image of the Mona Lisa, its creation date, and current location (e.g., "Louvre Museum").
        4.  **Expansion & Focus:** The user clicks on the "Mona Lisa" node. The graph animates smoothly, either re-centering on "Mona Lisa" as the new hub or expanding to show its primary connections (e.g., "depicts: Lisa Gherardini," "located in: Louvre Museum," "material: oil on poplar panel"). The `wikidata-mcp` server efficiently provides this next layer of data.
        5.  **Filtering Connections:** On a sidebar or contextual menu, the user sees filter options like "Relationship Type" (e.g., show only "influenced by," "student of," "patron of") and "Item Type" (e.g., "Person," "Artwork," "Location," "Organization").
            *   Example: The user unchecks "Artwork" and "Location" under "Item Type." The graph dynamically updates, hiding those nodes and their connections, re-querying the `wikidata-mcp` with these new filter parameters to keep the display clean and relevant.
        6.  **Exploring a Path:** The user sees a node for "Andrea del Verrocchio" (Leonardo's teacher). They click on it. The graph now focuses on Verrocchio. The user can then easily see Verrocchio's other students, thus discovering connections they might not have known. A "breadcrumb trail" or history panel shows their exploration path (e.g., Leonardo da Vinci -> Mona Lisa -> Leonardo da Vinci -> Andrea del Verrocchio).
        7.  **Changing Layout (Optional):** The user might have options to change the graph layout (e.g., hierarchical, force-directed) for different perspectives.
        8.  **Outcome:** The user has effortlessly navigated a complex web of information, visually understanding relationships and discovering new facts in an engaging way, all made possible by the responsive and structured data delivery from the `wikidata-mcp` server.

**Section 4: The Information - What does it need to handle?**

1.  **Information Needed:** What kinds of information does the program need to work with, store, or display?
    *   *Your Answer (List all types, following iterative refinement, considering data from Wikidata and the MCP server):*
        The program primarily works with data sourced from Wikidata, accessed and processed via the `wikidata-mcp` server. This includes:
        ▪ **Entities (Items):** Identified by unique QIDs (e.g., Q762 for Leonardo da Vinci). These represent people, places, organizations, concepts, artworks, events, etc.
        ▪ **Labels:** Human-readable names for entities in multiple languages (e.g., "Leonardo da Vinci" in English, "Léonard de Vinci" in French). The `wikidata-mcp` server should prioritize providing these.
        ▪ **Descriptions:** Short phrases disambiguating entities, in multiple languages.
        ▪ **Aliases (Also Known As):** Alternative names or search terms for entities, in multiple languages.
        ▪ **Properties:** Identified by PIDs (e.g., P31 for "instance of"). These define the type of relationship or attribute.
        ▪ **Statements (Claims):** The core data structure: an entity has a property with a value (e.g., "Leonardo da Vinci (Q762) -> educated at (P69) -> Andrea del Verrocchio (Q207410)"). The `wikidata-mcp` server will deliver these statements.
        ▪ **Values:** The data linked by a property. Values can be:
            *   Other Wikidata Items (creating the linked data graph)
            *   Strings (text)
            *   Dates and times (with precision)
            *   Quantities (numbers with units)
            *   Geographic coordinates (latitude, longitude)
            *   Monolingual text (e.g., for quotes)
            *   External identifiers (links to other databases)
            *   Media files (URLs to images, audio, video on Wikimedia Commons, facilitated by `wikidata-mcp`)
        ▪ **Qualifiers:** Additional information contextualizing a statement (e.g., start/end dates for a job, method used for a discovery). The `wikidata-mcp` server should make these accessible.
        ▪ **References (Sources):** Information about where the data for a statement came from. The `wikidata-mcp` may simplify or prioritize how these are presented.
        ▪ **Sitelinks:** Links from a Wikidata item to corresponding pages on Wikipedia, Wiktionary, etc.
        ▪ **Data Types:** The type of each value (e.g., item, string, quantity, date). The `wikidata-mcp` server will provide this to help the client render information correctly.
        ▪ **Thumbnails/Images:** URLs for representative images of items, ideally prioritized or optimized by the `wikidata-mcp`.

2.  **Data Relationships (Optional but helpful):** Do any pieces of information naturally belong together?
    *   *Your Answer (following iterative refinement, this is key for Wikidata):*
        Yes, this is fundamental to Wikidata and how the `wikidata-mcp` server will structure data for the client.
        *   **Core "Subject-Predicate-Object" Triples:** The most basic relationship is an Item (subject) linked by a Property (predicate) to a Value (object). For example:
            *   [Marie Curie] (Item) --- `educated at` (Property) ---> [University of Paris] (Value, which is also an Item).
            *   [Earth] (Item) --- `instance of` (Property) ---> [Planet] (Value, also an Item).
            *   [Mona Lisa] (Item) --- `creation date` (Property) ---> [1503] (Value, a Date).
        *   **Item-Centric Grouping:** All statements (properties and their values) about a single Item naturally belong together, forming the "page" or "profile" for that item. The `wikidata-mcp` would provide an endpoint like `/entity/{QID}` to fetch this grouped information.
        *   **Statement Context:** Qualifiers and References are directly tied to a specific statement, providing further context or substantiation for that particular claim. The `wikidata-mcp` should ensure these are retrievable with the statement.
        *   **Hierarchical & Network Relationships:** "Instance of" (P31) and "subclass of" (P279) create classification hierarchies. Other properties (e.g., "child of," "part of," "influenced by") create complex network or graph structures. The `wikidata-mcp` server will be optimized for traversing these relationships, for example, to generate a family tree or an organization chart.
        *   **Inverse Properties:** Many properties have logical inverses (e.g., "spouse" is its own inverse; "child of" has "parent of" as an inverse). The `wikidata-mcp` might pre-calculate or make querying inverse relationships seamless, so if the client asks for "children of X," it's as easy as asking for "parents of Y."
        *   **Time-based and Location-based Groupings:** Events can be grouped by date, items by location, allowing for timeline or map visualizations. The `wikidata-mcp` server could offer specialized queries for such groupings.

**Section 5: The Look & Feel - How should it generally seem?**

1.  **Overall Style:** Choose words that describe the general vibe.
    *   *Your Answer (following iterative refinement, reflecting a "superior" and modern tool):*
        **Modern, Clean, Intuitive, Insightful, Trustworthy, Responsive, Engaging, Discoverable, Accessible.**
        It should feel like a sophisticated yet approachable tool that invites exploration and makes complex information feel understandable and visually appealing. The speed and reliability of the `wikidata-mcp` server will directly contribute to the "Responsive" and "Trustworthy" feel.

2.  **Similar Programs (Appearance):** Are there any existing websites or apps whose look (not necessarily function) you like? Mentioning them helps the AI understand your visual preference.
    *   *Your Answer (following iterative refinement):*
        *   **Google's Knowledge Panels & Search Results:** For the clarity of information hierarchy, concise summaries, and clean presentation of facts.
        *   **Modern Wikipedia Interface (e.g., Vector 2022 skin):** For its readability, well-structured infoboxes, and good typography in presenting dense information.
        *   **Flourish.studio or Tableau Public Gallery:** For inspiration on interactive, engaging, and aesthetically pleasing data visualizations (charts, maps, network graphs).
        *   **Kumu.io:** For the fluid and interactive feel of its network graph visualizations and exploration.
        *   **Well-designed museum online collection portals:** For how they present individual items with rich metadata in an engaging and visually appealing way.
        *   **Notion.so or Craft.do:** For their clean, minimalist design, good use of whitespace, and focus on clear content presentation.

**Section 6: The Platform - Where will it be used?**

1.  **Primary Environment:** Where do you imagine most people using this program?
    *   *Your Primary Choice & any Secondary Choices (following iterative refinement):*
        *   **[X] On a Website (accessed through Chrome, Safari, Firefox, Edge, etc.) - Primary Choice.** This offers the broadest accessibility across devices and operating systems without requiring installation.
        *   *(Secondary Consideration):* The website should be highly responsive and work excellently on tablets, as these devices are well-suited for exploration and visual interaction. A Progressive Web App (PWA) approach could be considered for an app-like feel and potential offline caching of some user data.

2.  **(If Mobile App):** Does it need to work without an internet connection sometimes?
    *   *Your Answer (following iterative refinement, consider if offline Wikidata is feasible/useful with MCP):*
        Since the primary platform is a website and it relies on the online `wikidata-mcp` server for live, comprehensive data, **full offline functionality is not a primary goal.**
        However, for a superior experience, the client could implement:
        *   **Intelligent Caching:** Cache recently viewed items, search results, and the structure of saved visualizations. This would allow users to revisit their recent activity even with intermittent connectivity.
        *   **Saved State:** User-specific data like saved queries, bookmarked items, or personalization settings could be stored locally (e.g., in browser storage) or in a user account (if implemented) and accessible offline to some extent.
        The `wikidata-mcp` server itself is assumed to be always online; the client's offline capabilities would be about gracefully handling temporary connection loss for already fetched or user-saved data, not about browsing new Wikidata content offline.

**Section 7: The Rules & Boundaries - What are the non-negotiables?**

1.  **Must-Have Rules:** Are there any critical rules the program must follow?
    *   *Your Answer (following iterative refinement, consider data attribution, update frequency via MCP):*
        *   **Clear Data Attribution:** Always clearly state that the data is sourced from Wikidata (e.g., "Data from Wikidata, available under CC0") and provide easy links back to the source Wikidata items. The `wikidata-mcp` server must pass through the necessary information for this.
        *   **Prioritize Human-Readable Labels:** Default to showing clear, localized (based on user preference or browser settings) labels for items and properties. QIDs/PIDs should be easily accessible for experts but not the default display.
        *   **Transparency about Data Freshness:** If the `wikidata-mcp` server has a data update lag from live Wikidata, this should be subtly communicated (e.g., "Data updated as of [MCP sync date/time]").
        *   **High Performance & Responsiveness:** Interactions should feel fast and fluid. This is a core benefit expected from using the `wikidata-mcp` server.
        *   **User Privacy:** All user activity, if tracked for personalization or analytics, must be handled according to strong privacy principles and transparent policies.
        *   **Accessibility Standards:** The interface must be designed to be accessible to users with disabilities, adhering to guidelines like WCAG 2.1 Level AA.
        *   **No Misleading Inferences:** If the client performs any data aggregation or inference beyond what Wikidata directly states, this should be clearly distinguished from directly sourced facts.

2.  **Things to Avoid:** Is there anything the program should absolutely not do?
    *   *Your Answer (following iterative refinement):*
        *   **Never Display Raw, Unstyled SPARQL Queries to End Users:** The `wikidata-mcp` server and NexusInsight client are meant to abstract this away entirely for most users.
        *   **Avoid Overwhelming Users:** Don't dump massive amounts of raw data. Prioritize, summarize, and provide progressive disclosure of information.
        *   **No Direct Wikidata Editing (Initially):** For this version, the client should be read-only to focus on superior data access and exploration. Editing Wikidata is a complex process and out of scope for the initial "superior client" focused on consumption. (Future versions might consider a curated contribution pathway via the `wikidata-mcp`.)
        *   **Avoid Broken Links or Unresolved Entities:** The client should gracefully handle cases where expected data might be missing from Wikidata, ideally with help from the `wikidata-mcp` server to provide placeholders or explanations.
        *   **Don't Make Unsubstantiated Claims of Absolute Truth:** Always frame information as "data according to Wikidata."
        *   **Avoid Requiring Deep Technical Knowledge for Basic Use:** Core features must be usable by novices.
        *   **Avoid a Cluttered, Confusing, or Inconsistent Interface:** Maintain a clean, predictable, and intuitive design.
        *   **Avoid Storing Sensitive User Data Unnecessarily or Insecurely.**

**Section 8: Success Criteria - How do we know it's perfect?**

1.  **Definition of Done:** Describe 2-3 simple scenarios. If the program handles these scenarios exactly as described, you'd consider it a success for that part.
    *   *Your Scenarios (following iterative refinement, ensure these test "superior" aspects and MCP integration):*
        ▪ 1. **Novice User - Effortless Fact-Finding & Contextual Understanding:**
            A high school student needs to understand the key aspects of "Photosynthesis." They type "photosynthesis" into NexusInsight.
            *   **Expected Outcome:** They instantly see a clear, concise summary page (sourced from `wikidata-mcp`): a definition, key inputs (e.g., sunlight, water, carbon dioxide), outputs (e.g., glucose, oxygen), the type of process it is (e.g., metabolic process), and an illustrative diagram if available. Clicking on "glucose" smoothly transitions to the page for glucose, showing its chemical formula and role in biology. The student feels they quickly understood the basics and related concepts without feeling overwhelmed.

        ▪ 2. **Researcher - Complex Exploration & Targeted Data Extraction:**
            A historian is researching "female monarchs in 17th century Europe." They use the visual query builder to specify: "Show me all humans (female) who held the position of 'monarch' with start date within 1600-1699, and whose territory was 'located in continent: Europe'."
            *   **Expected Outcome:** NexusInsight, via the `wikidata-mcp` server, returns a list of these monarchs. The user can then choose to:
                *   View this list as an interactive timeline.
                *   Display them on a map of Europe based on their primary territory.
                *   Export the core data (Name, Reign Start, Reign End, Kingdom) as a CSV file.
                The researcher accomplishes this complex task in minutes without writing any code, and the results are visually explorable and easily exportable.

        ▪ 3. **Curious Explorer - Serendipitous Discovery through Visual Navigation:**
            A user starts by searching for "Star Wars." They see the main item page and then click to enter the "Interactive Visual Exploration of Connections" mode.
            *   **Expected Outcome:** They see a graph with "Star Wars" at the center, connected to characters, planets, directors, etc. They click on "Luke Skywalker," then on "Mark Hamill" (actor). From Mark Hamill, they notice a connection to "Batman: The Animated Series" (voice actor). They click on that, then on "Kevin Conroy." Within a few clicks, guided by clear visual cues and fast loading times (thanks to `wikidata-mcp`), they've serendipitously learned about voice acting connections they didn't set out to find, enjoying the process of discovery.

**Section 9: Inspirations & Comparisons - Learning from others**

1.  **Similar Programs (Functionality):** Are there any existing programs, websites, or apps that do something similar to what you envision (even if only partly)?
    *   *Your Answer (following iterative refinement, mention existing Wikidata tools):*
        *   **Wikidata Query Service (WDQS):** The official SPARQL endpoint; powerful but very technical.
        *   **Reasonator, SQID:** Tools that attempt to present a human-readable summary of a Wikidata item.
        *   **Scholia:** A service that creates scholarly profiles from Wikidata, focusing on researchers, publications, and organizations.
        *   **Wikipedia:** While text-based, its infoboxes and structured data elements (often sourced from Wikidata) provide a model for concise summaries.
        *   **Various custom Wikidata visualizers:** Many one-off tools exist for specific types of visualizations (e.g., family trees, timelines) but are not general-purpose clients.
        *   **Commercial Knowledge Graph Browsers (e.g., aspects of Neo4j Bloom, Cambridge Semantics AnzoGraph):** These show how graph data can be explored interactively, though they are not specific to Wikidata or typically free for public use in this way.
        The `wikidata-mcp` server aims to provide the robust backend that can power a client surpassing many of these in ease-of-use and integrated experience.

2.  **Likes & Dislikes:** For the programs listed above (or similar ones you know), what features or ways of doing things do you REALLY like? What do you REALLY dislike or find frustrating? This helps SPARC build something better.
    *   *Likes (following iterative refinement):*
        *   **WDQS:** The *potential* to query almost anything, direct access to the raw graph.
        *   **Reasonator/SQID:** The *concept* of an automated, multi-faceted summary page for an item; automatic inclusion of images.
        *   **Scholia:** Excellent, focused views for specific domains (e.g., a researcher's publication network); good aggregation of relevant data.
        *   **Wikipedia Infoboxes:** Their conciseness and scannability for key facts.
        *   **Modern Graph Visualization Tools (e.g., Kumu.io, Gephi):** Highly interactive, customizable, and aesthetically pleasing graph exploration; smooth animations and intuitive controls.
        *   **Google Knowledge Graph panels:** Clear presentation, good hierarchy of information.
    *   *Dislikes (following iterative refinement):*
        *   **WDQS:** Extremely high barrier to entry (SPARQL). Intimidating interface for non-programmers. Query timeouts and complexity limits can be frustrating even for experts. Unpredictable result formats.
        *   **Reasonator/SQID:** Often very slow to load or can fail to render. Information can be overwhelming and poorly prioritized. UI can feel dated and clunky.
        *   **General Wikidata tools:** Often lack intuitive visual query builders. Exporting data in truly user-friendly formats can be difficult. Navigation between items and understanding context can be disjointed. Lack of responsiveness or modern UI aesthetics.
        NexusInsight, leveraging the `wikidata-mcp` server, aims to combine the "likes" (power, rich information, focused views, great visualizations) while systematically addressing the "dislikes" (complexity, slowness, information overload, poor UI).

**Section 10: Future Dreams (Optional) - Where could this go?**

1.  **Nice-to-Haves:** Are there any features that aren't essential right now but would be great to add later?
    *   *Your Answer (following iterative refinement, think big for a superior client):*
        *   **Natural Language Querying (NLQ):** Allow users to ask questions in plain English (e.g., "Who were the female Nobel laureates in chemistry before 1950?"). This would likely involve an AI layer interfacing with the `wikidata-mcp` server's simplified query capabilities.
        *   **User Accounts & Personalization:** Allow users to save queries, create personal collections of items, customize their dashboard, and set persistent preferences. The `wikidata-mcp` server infrastructure could support storing these user profiles.
        *   **Collaborative Spaces:** Allow users to create shared "boards" or "collections" of Wikidata items and visualizations for group projects or discussions.
        *   **Alerts & Notifications:** Enable users to "watch" specific Wikidata items or query patterns and receive notifications about significant changes or new additions (would require the `wikidata-mcp` to track deltas).
        *   **AI-Driven "Insight Engine":** Proactively suggest interesting connections, anomalies, or patterns based on user exploration or broader data trends within Wikidata, processed by the `wikidata-mcp`.
        *   **Lightweight Data Quality Feedback Loop:** A simple way for users to flag potential data errors or suggest improvements they observe, which could be aggregated by the `wikidata-mcp` for review by Wikidata editors.
        *   **Embedded/Sharable Visualizations:** Allow users to generate embed codes for visualizations created in NexusInsight to include them in blogs, articles, or presentations.
        *   **Educational Modules & Guided Tours:** Curated walkthroughs demonstrating how to explore specific topics (e.g., "Tracing the history of an invention," "Understanding a dynasty's influence") using NexusInsight's features.
        *   **Developer API:** A simple API for other tools or developers to access the curated and simplified views provided by NexusInsight (essentially a higher-level API on top of `wikidata-mcp`'s direct offerings).

2.  **Long-Term Vision:** Any thoughts on how this program might evolve in the distant future?
    *   *Your Answer (following iterative refinement):*
        The long-term vision for NexusInsight is to become **the default, intuitive gateway for individuals and applications worldwide to not just access, but *truly understand, visualize, and utilize* the vast, interconnected knowledge within Wikidata.** It aims to:
        *   Empower universal discovery and learning by transforming raw linked data into meaningful, explorable, and actionable insights.
        *   Become an indispensable tool in education, research, journalism, and personal enrichment.
        *   Evolve into a "knowledge cartography" platform, where users can visually map, analyze, and share complex domains of knowledge.
        *   Potentially integrate or interoperate with other open knowledge graphs and datasets via an enhanced `wikidata-mcp` server, offering an even broader perspective on connected information.
        *   Foster a community of explorers who share their discoveries and contribute to a collective understanding of the world's knowledge.

**Section 11: Technical Preferences (Strictly Optional!)**
• Note: Our AI assistants are experts at choosing the best technical tools. Only fill this out if you have a very strong, specific reason for a particular choice (e.g., compatibility with an existing system you must use).
*(As the AI Product Architect, I suggest the following as informed considerations for SPARC, aligning with the "superior Wikidata client" concept and `wikidata-mcp` integration.)*

1.  **Specific Programming Language?**
    *   Language (and refined reasoning): For the **client-side (frontend)**, modern **JavaScript/TypeScript** utilizing a framework like React, Vue, or Svelte is highly recommended for building a rich, interactive, and responsive user interface. For any **client application backend** (if needed to manage user sessions, orchestrate complex calls to `wikidata-mcp`, etc., beyond what a pure frontend can do), Python (with Flask/FastAPI) or Node.js (with Express) would be suitable due to their strong communities, libraries for web development, and ease of integration.
    *   Reason (Mandatory if language specified): These choices promote rapid development, excellent user experience, large talent pools, and robust ecosystems of tools and libraries well-suited for data-rich web applications interacting with an external API like the `wikidata-mcp` server.

2.  **Specific Database?**
    *   Database (and refined reasoning): The primary data source is the `wikidata-mcp` server; NexusInsight itself should aim to be largely stateless. However, for features like user accounts, saved preferences, stored queries, or cached results, a **PostgreSQL** database is a robust and versatile choice. Alternatively, for simpler needs or if a NoSQL approach is preferred for user profile data, **MongoDB** could be considered.
    *   Reason (Mandatory if database specified): PostgreSQL offers reliability, data integrity, and powerful querying capabilities for relational user data. MongoDB offers flexibility for document-based storage of user profiles or settings. The choice depends on the complexity of user-specific data to be stored by the client application itself, separate from the main Wikidata content served by `wikidata-mcp`.

3.  **Specific Cloud Provider?**
    *   Provider (and refined reasoning): The choice should prioritize **low latency to the `wikidata-mcp` server** and offer robust, scalable services for web application hosting and (if needed) a client-side backend database. Leading providers like **AWS, Google Cloud Platform (GCP), or Azure** all offer suitable services (e.g., managed container services, serverless functions, CDNs, managed databases).
    *   Reason (Mandatory if provider specified): The selection should be based on a combination of factors: geographical proximity to the `wikidata-mcp` server (if known) to minimize network latency, cost-effectiveness for the expected scale, developer familiarity, and the specific managed services that would best support a highly available and performant web application. Strong CDN capabilities are essential for fast delivery of frontend assets.

**Final Check:**
• Have you answered all the questions in Sections 1-9 as clearly and detailed as possible, following the iterative refinement process for each? Yes.
• Have you used simple, everyday language suitable for the SPARC blueprint, while ensuring your underlying thinking captures the sophistication of a "superior" client? Yes, I believe so.
• Have you focused on the what and why, highlighting the benefits of leveraging the conceptualized `wikidata-mcp` server? Yes, this has been a core focus.

**(End of Blueprint Sections)**